import sys
import os
import json
import sqlite3
import requests
import urllib3
import time
import numpy as np
import re
import hashlib
import jieba
import jieba.posseg as pseg
from collections import defaultdict

# PyQt Core ç»„ä»¶ç”¨äºçº¿ç¨‹å’Œä¿¡å·
from PyQt5.QtCore import QThread, pyqtSignal, QMutex, QMutexLocker

# ================= é…ç½®ä¸ç¯å¢ƒ =================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
os.environ['CURL_CA_BUNDLE'] = ''

# API é…ç½® (ç¡¬ç¼–ç  Key)
API_KEY = "sk-fXM4W0CdcKnNp3NVDfF85f2b90284b11AfDdF9F5627f627b"

# 1. Embedding API
EMBEDDING_API_URL = "https://aiplus.airchina.com.cn:18080/v1/embeddings" 
EMBEDDING_MODEL_NAME = "bge-m3"

# 2. Rerank API
RERANK_API_URL = "https://aiplus.airchina.com.cn:18080/v1/rerank" 
RERANK_MODEL_NAME = "bge-reranker-v2-m3"

# 3. DeepSeek/LLM API
DEEPSEEK_API_URL = "https://aiplus.airchina.com.cn:18080/v1/chat/completions"
DEEPSEEK_V3_MODEL_NAME = "DeepSeek-V3"

# ================= System Prompts =================
REWRITE_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šçº§ RAG ç³»ç»Ÿä¸­çš„ã€ŒQuery Rewrite æ¨¡å—ã€ã€‚
ä½ çš„èŒè´£ä¸æ˜¯å›ç­”é—®é¢˜ï¼Œè€Œæ˜¯ï¼š
å°†ç”¨æˆ·è¾“å…¥çš„ã€Œç®€çŸ­ã€æ¨¡ç³Šæˆ–å£è¯­åŒ–æŸ¥è¯¢ã€
é‡å†™ä¸ºä¸€ä¸ªã€Œè¯­ä¹‰æ¸…æ™°ã€ä¿¡æ¯å¯†åº¦é«˜ã€é€‚åˆå‘é‡æ£€ç´¢ä¸ reranker åˆ¤æ–­ç›¸å…³æ€§çš„æŸ¥è¯¢ã€ã€‚

ä½ å¿…é¡»éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
1. ä¿æŒç”¨æˆ·åŸå§‹æ„å›¾ä¸å˜ï¼Œä¸å¼•å…¥ä¸å­˜åœ¨çš„äº‹å®
2. å¯¹æ¦‚å¿µè¿›è¡Œåˆç†å±•å¼€ä¸åŒä¹‰è¡¥å…¨ï¼ˆsemantic expansionï¼‰
3. è¾“å‡ºçš„æŸ¥è¯¢å¿…é¡»æ›´æœ‰åˆ©äºæŠ€æœ¯æ–‡æ¡£ã€è®ºæ–‡ã€è¯´æ˜æ€§æ–‡æœ¬çš„æ£€ç´¢
4. ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€åˆ†ææˆ–å¤šç‰ˆæœ¬ç»“æœ
5. åªè¾“å‡ºä¸€æ¡é‡å†™åçš„æŸ¥è¯¢æ–‡æœ¬

é‡å†™è§„åˆ™ï¼š
- å¦‚æœç”¨æˆ·æŸ¥è¯¢è¿‡çŸ­ï¼ˆâ‰¤3 ä¸ªè¯ï¼‰ï¼Œå¿…é¡»è¿›è¡Œè¯­ä¹‰æ‰©å±•
- å¦‚æœæŸ¥è¯¢åŒ…å«æ­§ä¹‰è¯ï¼ˆå¦‚ modelã€trainã€dataã€method ç­‰ï¼‰ï¼Œéœ€æ ¹æ®ã€ŒæŠ€æœ¯æ–‡æ¡£æ£€ç´¢ã€åœºæ™¯è¿›è¡Œæ¶ˆæ­§
- ä¼˜å…ˆä½¿ç”¨å®Œæ•´è‡ªç„¶è¯­è¨€æè¿°ï¼Œè€Œä¸æ˜¯å…³é”®è¯å †å 
- è¾“å‡ºé•¿åº¦å»ºè®®ä¸º 1 å¥è¯ï¼Œæœ€å¤šä¸è¶…è¿‡ 2 å¥è¯"""

DEEPSEEK_R1_BASE_PROMPT = """ğŸ¯ã€è§’è‰²å®šä¹‰ã€‘
ä½ æ˜¯ä¸€ä¸ª RAG Final Answer Composerï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå™¨ï¼‰ã€‚
ä½ çš„ä»»åŠ¡ ä¸æ˜¯æ£€ç´¢ã€ä¸æ˜¯æ’åºã€ä¸æ˜¯çŒœæµ‹ï¼Œè€Œæ˜¯ï¼š
ä¸¥æ ¼åŸºäºå·²æä¾›çš„å¬å›ç»“æœï¼Œå¯¹ç”¨æˆ·é—®é¢˜ç”Ÿæˆæœ€ç»ˆã€å¯è¯»ã€å‡†ç¡®çš„å›ç­”ã€‚

ğŸ“¥ã€è¾“å…¥è¯´æ˜ã€‘
ä½ å°†æ”¶åˆ°ä¸€ä¸ªç»“æ„åŒ–è¾“å…¥ï¼ŒåŒ…å«ï¼š
1. query: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
2. retrieved_chunks: åŒ…å«æ¥è‡ª [VECTOR] (å‘é‡å¬å›) å’Œ [JSON_Source] (åŸæ–‡ç¡¬åŒ¹é…) çš„æ··åˆå†…å®¹ã€‚

ğŸ”’ã€å¼ºåˆ¶çº¦æŸã€‘
1ï¸âƒ£ äº‹å®æ¥æºçº¦æŸï¼ˆé˜²å¹»è§‰ï¼‰
âŒ ç¦æ­¢ ä½¿ç”¨ä»»ä½•å¤–éƒ¨çŸ¥è¯†
âŒ ç¦æ­¢ è¡¥å……æœªåœ¨ retrieved_chunks ä¸­å‡ºç°çš„äº‹å®
âœ… åªå…è®¸ åŸºäºæä¾›å†…å®¹è¿›è¡Œå½’çº³ã€é‡å†™ã€æ€»ç»“
å¦‚æœè¯æ®ä¸è¶³ï¼šå¿…é¡»æ˜ç¡®è¯´æ˜ã€Œå½“å‰å¬å›å†…å®¹ä¸è¶³ä»¥å®Œæ•´å›ç­”è¯¥é—®é¢˜ã€

2ï¸âƒ£ å†…å®¹ä½¿ç”¨è§„åˆ™ï¼ˆé˜²é—æ¼ï¼‰
ä¼˜å…ˆä½¿ç”¨ Rank é å‰çš„å†…å®¹ã€‚
æ³¨æ„åŒºåˆ†æ¥æºï¼š[JSON_Source] æ¥æºçš„å†…å®¹ç›´æ¥æ¥è‡ªåŸå§‹æ–‡æ¡£ï¼Œå…·æœ‰æœ€é«˜çš„äº‹å®å‚è€ƒä»·å€¼ã€‚

3ï¸âƒ£ å™ªå£°å¤„ç†è§„åˆ™
å…è®¸ä½ ï¼šä¿®å¤æ–­è¡Œã€åˆå¹¶è¢«æ‹†æ•£çš„å¥å­ã€å»é™¤æ˜æ˜¾ä¹±ç 
âŒ ä¸å…è®¸â€œåˆç†çŒœæµ‹â€ç¼ºå¤±å†…å®¹

âœï¸ã€è¾“å‡ºè¦æ±‚ã€‘
è¾“å‡ºå¿…é¡»æ»¡è¶³ï¼š
âœ… è¯­è¨€æ¸…æ™°ã€æŠ€æœ¯å‡†ç¡®
âœ… **å¿…é¡»ä½¿ç”¨ Markdown æ ¼å¼ï¼ŒåŒ…å«æ¸…æ™°çš„æ®µè½ã€åˆ—è¡¨å’ŒåŠ ç²—**
âœ… ä¸ç›´æ¥å¤§æ®µå¤åˆ¶åŸæ–‡ï¼ˆå…è®¸çŸ­å¼•ç”¨ï¼‰

âš ï¸ã€å¤±è´¥å…œåº•ç­–ç•¥ã€‘
å¦‚æœæ‰€æœ‰ retrieved_chunks ä¸ query ç›¸å…³æ€§éƒ½å¾ˆå¼±ï¼Œå¿…é¡»è¾“å‡ºï¼šâ€œæ ¹æ®å½“å‰å¬å›çš„æ–‡æ¡£å†…å®¹ï¼Œæ— æ³•å¯¹è¯¥é—®é¢˜ç»™å‡ºå¯é å›ç­”ã€‚â€"""

# ================= æ ¸å¿ƒå·¥å…·å‡½æ•° =================
def cosine_similarity(vec1, vec2):
    try:
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return np.dot(vec1, vec2) / (norm1 * norm2)
    except Exception:
        return 0.0

def get_text_hash(text):
    """ç”Ÿæˆæ–‡æœ¬çš„ SHA256 å“ˆå¸Œï¼Œç”¨äºä¸¥æ ¼å»é‡"""
    clean_text = text.strip().lower()
    return hashlib.sha256(clean_text.encode('utf-8')).hexdigest()

def extract_keywords_with_jieba(query, stopwords=None, airline_dict=None, top_n=5):
    """
    ã€ä¸‰æ­¥èµ°ç­–ç•¥åº”ç”¨ã€‘
    Step 1: å­—å…¸ä¼˜å…ˆ (Airline Dict) -> æƒé‡ 3 (æœ€é«˜)
    Step 2: æ­£åˆ™ä¼˜å…ˆ (Flight No/Code) -> æƒé‡ 3
    Step 3: Jieba NLP -> æƒé‡ 1-2
    """
    if not jieba:
        return query.split() # Fallback
    
    keywords = []
    query_lower = query.lower()
    
    # é¢„å¤„ç† stopwords set
    stop_set = set()
    if stopwords:
        for sw in stopwords:
            stop_set.add(sw.strip().lower())

    # --- Step 1: å­—å…¸ä¼˜å…ˆ (Exact Match in Query) ---
    if airline_dict:
        for airline in airline_dict:
            # ç®€å•åŒ…å«æ£€æµ‹
            if airline.lower() in query_lower:
                # æ‰¾åˆ°èˆªå¸åï¼Œç›´æ¥ä½œä¸ºé«˜æƒé‡å…³é”®è¯
                keywords.append((airline, 3))

    # --- Step 2: æ­£åˆ™ä¼˜å…ˆ (Regex for Codes) ---
    # åŒ¹é… CA123, B737, A320 ç­‰
    code_pattern = r'[A-Za-z]{2,3}\d{3,4}' 
    codes = re.findall(code_pattern, query)
    for code in codes:
        keywords.append((code, 3))

    # --- Step 3: NLP (Jieba) ---
    words = pseg.cut(query)
    for w in words:
        word = w.word.strip()
        flag = w.flag
        
        if len(word) < 2: continue 
        if word.lower() in stop_set: continue
        
        # é¿å…é‡å¤æ·»åŠ  Step 1/2 å·²ç»æ‰¾åˆ°çš„è¯
        if any(k[0].lower() == word.lower() for k in keywords):
            continue

        if flag.startswith('n') or flag == 'eng': # åè¯æˆ–è‹±æ–‡
            keywords.append((word, 2)) # ç¨ä½æƒé‡
        elif flag.startswith('v'): # åŠ¨è¯
            keywords.append((word, 1)) # æœ€ä½æƒé‡
        # å…¶ä»–è¯æ€§å¿½ç•¥

    # æŒ‰æƒé‡æ’åºå¹¶å»é‡
    keywords.sort(key=lambda x: x[1], reverse=True)
    seen = set()
    result = []
    for k, score in keywords:
        if k not in seen:
            result.append(k)
            seen.add(k)
            
    return result[:top_n]

def is_precise_intent(query):
    """
    åŠ¨æ€è·¯ç”±é€»è¾‘ï¼šæ£€æµ‹æ˜¯å¦åŒ…å«å¤§å†™å­—æ¯+æ•°å­—çš„ç»„åˆï¼ˆå¦‚ CA1234, B737 ç­‰ï¼‰
    """
    pattern = r'[A-Z]{2,3}\d{3,4}'
    return bool(re.search(pattern, query))

# ================= PageIndex Loader =================
class PageIndexLoader:
    def __init__(self):
        self.index = {}          
        self.ordered_ids = []    
        self.is_loaded = False

    def load_json(self, json_path):
        if not json_path or not os.path.exists(json_path):
            return False, "æ–‡ä»¶ä¸å­˜åœ¨"
        
        try:
            with open(json_path, 'r', encoding='utf-8-sig') as f:
                data = json.load(f)
            
            self.index = {}
            self.ordered_ids = []
            
            root_structure = data.get("structure", []) if isinstance(data, dict) else data
            
            for item in root_structure:
                self._traverse(item, parent_path=[])
            
            self.is_loaded = True
            return True, f"æˆåŠŸåŠ è½½ PageIndexï¼ŒåŒ…å« {len(self.index)} ä¸ªèŠ‚ç‚¹"
        except Exception as e:
            return False, f"åŠ è½½å¼‚å¸¸: {str(e)}"

    def _traverse(self, node, parent_path):
        current_title = node.get("title", "")
        node_id = str(node.get("node_id", "")) 
        current_path = parent_path + [current_title]
        
        if node_id:
            self.index[node_id] = {
                "title": current_title,
                "text": node.get("text", ""),
                "summary": node.get("summary", ""),
                "path": current_path,
                "raw_node": node 
            }
            self.ordered_ids.append(node_id)
        
        if "nodes" in node and isinstance(node["nodes"], list):
            for child in node["nodes"]:
                self._traverse(child, current_path)

    def get_node(self, node_id):
        return self.index.get(str(node_id))

# ================= Worker: JSON Hard Query (ç‹¬ç«‹çº¿ç¨‹) =================
class JsonHardQueryWorker(QThread):
    finished_signal = pyqtSignal(list, str) # results, status_msg

    def __init__(self, json_path, keywords):
        super().__init__()
        self.json_path = json_path
        self.keywords = keywords
        self._is_interrupted = False

    def stop(self):
        self._is_interrupted = True

    def run(self):
        if not self.json_path or not os.path.exists(self.json_path) or not self.keywords:
            self.finished_signal.emit([], "JSON è·¯å¾„æ— æ•ˆæˆ–æ— å…³é”®è¯")
            return

        results = []
        try:
            with open(self.json_path, 'r', encoding='utf-8-sig') as f:
                data = json.load(f)
            
            if self._is_interrupted: return 

            structure = data.get("structure", []) if isinstance(data, dict) else data
            
            def traverse_search(node, path_stack):
                if self._is_interrupted: return

                current_title = node.get("title", "æœªå‘½åç« èŠ‚")
                current_text = node.get("text", "")
                node_id = str(node.get("node_id", "unknown"))
                current_path = path_stack + [current_title]
                
                text_lower = current_text.lower()
                hit_count = 0
                for kw in self.keywords:
                    if kw.lower() in text_lower:
                        hit_count += 1
                
                if hit_count > 0:
                    if len(current_text) > 10:
                        path_str = " > ".join(current_path)
                        # åŸºç¡€åˆ† + å‘½ä¸­å¥–åŠ±
                        score = 10.0 + (hit_count * 2.0)
                        
                        results.append({
                            "id": node_id,
                            "content": current_text,
                            "path": path_str,
                            "score": score,
                            "hit_count": hit_count,
                            "source": "JSON_Source" 
                        })

                if "nodes" in node and isinstance(node["nodes"], list):
                    for child in node["nodes"]:
                        traverse_search(child, current_path)

            for item in structure:
                if self._is_interrupted: break
                traverse_search(item, [])
            
            if self._is_interrupted:
                self.finished_signal.emit([], "JSON æŸ¥è¯¢å·²ä¸­æ–­")
                return

            results.sort(key=lambda x: x['score'], reverse=True)
            top_results = results[:20] 
            
            self.finished_signal.emit(top_results, f"JSON åŸæ–‡æ£€ç´¢å‘½ä¸­: {len(top_results)} æ¡ (å…³é”®è¯: {self.keywords})")
            
        except Exception as e:
            self.finished_signal.emit([], f"JSON æŸ¥è¯¢å¼‚å¸¸: {str(e)}")

# ================= Worker: Recall + RRF Fusion =================
class RecallWorker(QThread):
    log_signal = pyqtSignal(str)          
    result_signal = pyqtSignal(list)      
    summary_signal = pyqtSignal(str)      
    finish_signal = pyqtSignal(bool)      

    def __init__(self, query_text, db_path, json_path, search_mode="smart", summary_model="DeepSeek-R1", 
                 doc_type="ä¸æŒ‡å®šç±»å‹", stopwords=None, airline_names=None, chunk_limit=40):
        super().__init__()
        self.original_query = query_text 
        self.search_query = query_text   
        self.db_path = db_path
        self.json_path = json_path
        self.search_mode = search_mode 
        self.summary_model = summary_model 
        self.doc_type = doc_type 
        self.stopwords = stopwords if stopwords else []
        self.airline_names = airline_names if airline_names else [] # Step 1 Dictionary
        self.chunk_limit = chunk_limit # åŠ¨æ€æµæ§å‚æ•°
        
        self.page_index = PageIndexLoader()
        self.json_search_results = [] 
        
        self._is_interrupted = False
        self._json_worker = None

    def stop(self):
        """å¤–éƒ¨è°ƒç”¨ä»¥åœæ­¢ä»»åŠ¡"""
        self.log("ğŸ›‘ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ä¸­æ–­ä»»åŠ¡...")
        self._is_interrupted = True
        if self._json_worker:
            self._json_worker.stop()
            self._json_worker.wait(100) 

    def log(self, msg):
        timestamp = time.strftime("%H:%M:%S")
        self.log_signal.emit(f"[{timestamp}] {msg}")

    def on_json_search_finished(self, results, msg):
        self.json_search_results = results
        self.log(f"ğŸ“„ {msg}")

    # --- Step 0: Query Rewrite ---
    def rewrite_query(self, original_query):
        if self._is_interrupted: return None

        if self.search_mode == "precise":
            self.log("â© ç²¾å‡†æ¨¡å¼ï¼šè·³è¿‡æŸ¥è¯¢é‡å†™")
            return original_query

        self.log(f"ğŸ§  æ­£åœ¨è¯·æ±‚ DeepSeek-V3 è¿›è¡Œè¯­ä¹‰é‡å†™ (ç±»å‹åå¥½: {self.doc_type})...")
        
        doc_type_hint = ""
        if self.doc_type and self.doc_type != "ä¸æŒ‡å®šç±»å‹":
            doc_type_hint = f"\n\n[Important Context]: The user explicitly expects content from document type: '{self.doc_type}'. Please refine the query to imply this context."

        headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {API_KEY}' }
        messages = [
            {"role": "system", "content": REWRITE_SYSTEM_PROMPT},
            {"role": "user", "content": f"ç”¨æˆ·æŸ¥è¯¢ï¼š\n{original_query}{doc_type_hint}\n\nè¯·è¾“å‡ºé‡å†™åçš„æŸ¥è¯¢ï¼š"}
        ]
        payload = {
            "model": DEEPSEEK_V3_MODEL_NAME, 
            "messages": messages,
            "temperature": 0.7, 
            "stream": False     
        }

        try:
            start_time = time.time()
            if self._is_interrupted: return None
            
            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, verify=False, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                content = data['choices'][0]['message']['content'].strip()
                content = content.replace('"', '').replace("'", "")
                
                self.log(f"âœ… Rewrite å®Œæˆ ({time.time() - start_time:.2f}s)")
                self.log(f"   Original: {original_query}")
                self.log(f"   Rewritten: {content}")
                return content
            else:
                self.log(f"âš ï¸ Rewrite API è¿”å›é”™è¯¯: {response.status_code}ï¼Œå°†ä½¿ç”¨åŸå§‹æŸ¥è¯¢ã€‚")
                return original_query
        except Exception as e:
            self.log(f"âš ï¸ Rewrite è°ƒç”¨å¼‚å¸¸: {str(e)}ï¼Œå°†ä½¿ç”¨åŸå§‹æŸ¥è¯¢ã€‚")
            return original_query

    # --- Step 1: Embedding ---
    def get_remote_embedding(self, text):
        if self._is_interrupted: return None
        self.log(f"ğŸ“¡ æ­£åœ¨è®¡ç®—å‘é‡ Embedding: {text[:30]}...")
        headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {API_KEY}' }
        payload = { "model": EMBEDDING_MODEL_NAME, "input": [text] }
        
        try:
            response = requests.post(EMBEDDING_API_URL, headers=headers, json=payload, verify=False, timeout=30)
            if response.status_code == 200:
                data = response.json()
                if 'data' in data and len(data['data']) > 0:
                    return data['data'][0]['embedding']
        except Exception as e:
            self.log(f"âŒ Embedding ç½‘ç»œå¼‚å¸¸: {str(e)}")
        return None

    # --- Step 2: Rerank API ---
    def rerank_with_bge(self, query, candidates_text_list):
        if not candidates_text_list or self._is_interrupted:
            return []
        
        rerank_query = query
        if self.doc_type and self.doc_type != "ä¸æŒ‡å®šç±»å‹":
            rerank_query = f"{query} (Prefer document type: {self.doc_type})"
            self.log(f"âš–ï¸ Reranker ä½¿ç”¨å¢å¼º Query: {rerank_query}")

        self.log(f"âš–ï¸ Reranker ({RERANK_MODEL_NAME}) æ­£åœ¨é‡æ’ {len(candidates_text_list)} æ¡æ•°æ®...")
        headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {API_KEY}' }
        
        payload = {
            "model": RERANK_MODEL_NAME,
            "query": rerank_query, 
            "documents": candidates_text_list 
        }

        try:
            start_time = time.time()
            if self._is_interrupted: return None
            
            response = requests.post(RERANK_API_URL, headers=headers, json=payload, verify=False, timeout=120)
            
            if response.status_code == 200:
                data = response.json()
                scores = [0.0] * len(candidates_text_list)
                
                if "results" in data:
                    for res in data["results"]:
                        idx = res.get("index")
                        score = res.get("relevance_score", 0.0)
                        if idx is not None and 0 <= idx < len(scores):
                            scores[idx] = score
                elif isinstance(data, list):
                     scores = data

                self.log(f"âœ… Reranker å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")
                return scores
            else:
                self.log(f"âš ï¸ Reranker è¯·æ±‚å¤±è´¥: {response.status_code}")
                return None
        except Exception as e:
            self.log(f"âš ï¸ Reranker è°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None

    # --- Step 4: LLM Summary ---
    def call_deepseek_summary(self, user_original_query, top_results):
        if self._is_interrupted: return

        target_model = self.summary_model 
        self.log(f"ğŸ§  æ­£åœ¨è¯·æ±‚ {target_model} ç”Ÿæˆæœ€ç»ˆå›ç­” (Stream=True)...")
        self.summary_signal.emit(f"> ğŸš€ **{target_model} å·²è¿æ¥ï¼Œå‡†å¤‡ç”Ÿæˆ...**\n\n")

        current_system_prompt = DEEPSEEK_R1_BASE_PROMPT
        if self.doc_type and self.doc_type != "ä¸æŒ‡å®šç±»å‹":
            doc_type_constraint = f"""
\nâš ï¸ã€æ–‡æ¡£ç±»å‹å¼ºåˆ¶åå¥½ã€‘
ç”¨æˆ·æœŸæœ›çš„ç­”æ¡ˆä¸»è¦æ¥è‡ªæ–‡æ¡£ç±»å‹ï¼šã€{self.doc_type}ã€‘ã€‚
1. å›ç­”æ—¶è¯·ä¼˜å…ˆå‚è€ƒè¯¥ç±»å‹çš„å†…å®¹ã€‚
2. ä½†å¦‚æœè·¨ç±»å‹å†…å®¹æ˜æ˜¾æœ‰åŠ©äºå›ç­”é—®é¢˜ï¼Œè¯·åˆç†è¡¥å……ã€‚
"""
            current_system_prompt += doc_type_constraint

        context_str = ""
        for item in top_results:
            source_tag = item.get('source', 'VECTOR')
            context_str += f"""
---
[Rank {item['rank']}] [Source: {source_tag}] (RRF: {item['final_score']:.4f})
Section Path: {item['path']}
Content:
{item['content']}
"""
        
        user_prompt_content = f"Query: {user_original_query}\n\nRetrieved Chunks:{context_str}"

        headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {API_KEY}' }
        payload = {
            "model": target_model, 
            "messages": [
                {"role": "system", "content": current_system_prompt},
                {"role": "user", "content": user_prompt_content}
            ],
            "stream": True,
            "temperature": 0.6
        }

        try:
            if self._is_interrupted: return

            response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload, verify=False, stream=True, timeout=120)
            
            if response.status_code == 200:
                full_reasoning = ""
                full_content = ""
                is_thinking_logged = False
                
                for line in response.iter_lines():
                    if self._is_interrupted: 
                        self.log("ğŸ›‘ æ€»ç»“ç”Ÿæˆå·²ä¸­æ–­")
                        self.summary_signal.emit("\n\n[ç”¨æˆ·ç»ˆæ­¢äº†ç”Ÿæˆ]")
                        break

                    if line:
                        decoded_line = line.decode('utf-8')
                        if decoded_line.startswith("data: "):
                            data_str = decoded_line[6:] 
                            if data_str.strip() == "[DONE]":
                                break
                            try:
                                json_chunk = json.loads(data_str)
                                delta = json_chunk['choices'][0]['delta']
                                
                                current_reasoning_delta = delta.get('reasoning_content', '')
                                current_content_delta = delta.get('content', '')
                                updated = False
                                
                                if current_reasoning_delta:
                                    if not is_thinking_logged:
                                        self.log("ğŸ§  æ£€æµ‹åˆ°æ€ç»´é“¾ (CoT)ï¼Œæ­£åœ¨æ€è€ƒ...")
                                        is_thinking_logged = True
                                    full_reasoning += current_reasoning_delta
                                    updated = True
                                
                                if current_content_delta:
                                    full_content += current_content_delta
                                    updated = True

                                if updated:
                                    formatted_output = ""
                                    if full_reasoning:
                                        clean_reasoning = full_reasoning.replace('\n', '\n> ')
                                        formatted_output += f"> ğŸ§  **Thinking Process:**\n> {clean_reasoning}\n\n"
                                    
                                    if full_reasoning and full_content:
                                        formatted_output += "---\n\n" 
                                        
                                    if full_content:
                                        formatted_output += f"{full_content}"
                                        
                                    self.summary_signal.emit(formatted_output)
                            except Exception:
                                continue
                if not self._is_interrupted:
                    self.log(f"âœ… {target_model} æ€»ç»“ç”Ÿæˆå®Œæ¯•")
            else:
                self.log(f"âŒ API é”™è¯¯: {response.status_code}")
                self.summary_signal.emit(f"âš ï¸ æ— æ³•ç”Ÿæˆæ€»ç»“: API Error {response.status_code}")

        except Exception as e:
            self.log(f"âŒ DeepSeek è°ƒç”¨å¼‚å¸¸: {str(e)}")
            self.summary_signal.emit(f"âš ï¸ æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}")

    # --- æ ¸å¿ƒç®—æ³•: RRF Fusion ---
    def apply_rrf_fusion(self, vector_items, json_items, k=60):
        fused_scores = defaultdict(float)
        item_map = {}
        
        # 1. Vector Results
        for rank, item in enumerate(vector_items):
            doc_id = item['id']
            item_map[doc_id] = item
            fused_scores[doc_id] += 1.0 / (k + rank + 1)
            
        # 2. JSON Results (Boost Logic)
        is_precise = is_precise_intent(self.original_query)
        json_boost = 1.0
        
        # --- ç­–ç•¥è°ƒæ•´: ä¹¦ç±æ¨¡å¼ä¸‹é™ä½ JSON å…³é”®è¯æƒé‡ ---
        is_book_mode = self.doc_type in ["ä¹¦ç±/æ•™æ", "é•¿ç¯‡è®ºæ–‡"]
        
        if self.search_mode == 'precise':
            json_boost = 5.0 
        elif self.search_mode == 'smart':
            if is_book_mode:
                self.log("ğŸ“š æ£€æµ‹åˆ°ä¹¦ç±/é•¿æ–‡æ¡£æ¨¡å¼ï¼šä¸»åŠ¨é™ä½å…³é”®è¯æƒé‡ï¼Œä¼˜å…ˆè¯­ä¹‰å¬å›")
                json_boost = 0.5  # é™æƒï¼Œé˜²æ­¢ä¹¦ç±ä¸­éå®šä¹‰çš„å…³é”®è¯å¹²æ‰°
            elif is_precise:
                self.log("ğŸ’¡ åŠ¨æ€è·¯ç”±: æ£€æµ‹åˆ°ç²¾ç¡®ä»£ç /èˆªç­å·ï¼Œè‡ªåŠ¨æå‡ JSON æƒé‡")
                json_boost = 3.0 
            else:
                json_boost = 1.0
        elif self.search_mode == 'fuzzy':
            json_boost = 0.5 

        for rank, item in enumerate(json_items):
            doc_id = item['id']
            if doc_id not in item_map:
                item_map[doc_id] = item
                item_map[doc_id]['debug_score'] = "JSON_New"
            
            fused_scores[doc_id] += json_boost * (1.0 / (k + rank + 1))
            if "JSON" not in item_map[doc_id].get('source', ''):
                item_map[doc_id]['source'] = "MIXED (Vec+JSON)"

        # 3. Sort
        sorted_doc_ids = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)
        
        # 4. Fingerprint Deduplication
        final_results = []
        seen_fingerprints = set()
        
        for doc_id in sorted_doc_ids:
            item = item_map[doc_id]
            item['final_score'] = fused_scores[doc_id]
            
            content_fingerprint = get_text_hash(item.get('content', ''))
            
            if content_fingerprint in seen_fingerprints:
                continue
            
            seen_fingerprints.add(content_fingerprint)
            final_results.append(item)
            
        return final_results

    def run(self):
        try:
            self._is_interrupted = False

            # 0. Load PageIndex
            has_pageindex = False
            if self.json_path:
                self.log(f"åŠ è½½ PageIndex: {os.path.basename(self.json_path)}...")
                success, msg = self.page_index.load_json(self.json_path)
                if success:
                    has_pageindex = True
                else:
                    self.log(f"âš ï¸ PageIndex åŠ è½½å¤±è´¥: {msg}")
            
            if self._is_interrupted: return

            # --- Concurrent Step: JSON Hard Query (Three-Step Enabled) ---
            self._json_worker = None
            if self.json_path and self.search_mode != 'fuzzy': 
                # ã€Updateã€‘ä¼ å…¥ airline_names å’Œ stopwords
                keywords = extract_keywords_with_jieba(
                    self.original_query, 
                    self.stopwords, 
                    self.airline_names
                )
                self.log(f"ğŸ” [ä¸‰æ­¥èµ°ç­–ç•¥] æå–å…³é”®è¯: {keywords}")
                
                if keywords:
                    self.log("ğŸš€ å¯åŠ¨ JSON åŸæ–‡ç¡¬æŸ¥è¯¢çº¿ç¨‹...")
                    self._json_worker = JsonHardQueryWorker(self.json_path, keywords)
                    self._json_worker.finished_signal.connect(self.on_json_search_finished)
                    self._json_worker.start()
            
            if self._is_interrupted: return

            # --- Step 0: Query Rewrite ---
            rewritten = self.rewrite_query(self.original_query)
            if self._is_interrupted: return
            self.search_query = rewritten if rewritten else self.original_query

            # --- Step 1: Query Vector ---
            vector_candidates = []
            
            query_vec_list = self.get_remote_embedding(self.search_query)
            if self._is_interrupted: return

            if query_vec_list and os.path.exists(self.db_path):
                query_vec_np = np.array(query_vec_list, dtype=np.float32)

                self.log(f"ğŸ“‚ æ­£åœ¨è¿æ¥æ•°æ®åº“: {os.path.basename(self.db_path)}")
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cursor.execute("SELECT id, embedding, section_id FROM vectors")
                rows = cursor.fetchall()
                
                raw_candidates = []
                for row in rows:
                    if self._is_interrupted: break 
                    v_id, emb_json, sec_id_db = row
                    try:
                        doc_vec = np.array(json.loads(emb_json), dtype=np.float32)
                        score = cosine_similarity(query_vec_np, doc_vec)
                        raw_candidates.append({"id": v_id, "vec_score": score, "section_id": str(sec_id_db)})
                    except: continue

                if self._is_interrupted: 
                    conn.close()
                    return

                raw_candidates.sort(key=lambda x: x["vec_score"], reverse=True)
                
                # ã€æµæ§å…³é”®ç‚¹ã€‘ä½¿ç”¨ chunk_limit è¿›è¡Œæˆªæ–­ï¼Œé˜²æ­¢ Reranker è¿‡è½½
                current_limit = self.chunk_limit
                self.log(f"âš¡ [æµæ§] é™åˆ¶å‘é‡å¬å›æ•°é‡ä¸º: {current_limit} (Mode: {self.doc_type})")
                top_candidates_raw = raw_candidates[:current_limit] 
                
                rerank_input_texts = []
                
                for item in top_candidates_raw:
                    if self._is_interrupted: break
                    sec_id = item["section_id"]
                    node_info = self.page_index.get_node(sec_id) if has_pageindex else None
                    
                    raw_text = ""
                    path_str = ""
                    summary_text = ""

                    if node_info:
                        raw_text = node_info['text']
                        path_str = " > ".join(node_info['path'])
                        summary_text = node_info.get('summary', '')
                    else:
                        cursor.execute("SELECT embedding_text, original_snippet, section_path FROM documents WHERE id=?", (sec_id,))
                        db_row = cursor.fetchone()
                        if db_row:
                            emb_summary = db_row[0] if db_row[0] else ""
                            raw_detail = db_row[1] if db_row[1] else ""
                            raw_text = f"ã€å†…å®¹æ‘˜è¦ã€‘ï¼š{emb_summary}\n\nã€åŸå§‹æ•°æ®ã€‘ï¼š{raw_detail}"
                            path_str = str(db_row[2])
                        else:
                            continue

                    rerank_input_texts.append(f"Section Path: {path_str}\nContent: {raw_text}")
                    display_content = f"[Summary]\n{summary_text}\n\n[Text]\n{raw_text}" if summary_text else raw_text
                    
                    vector_candidates.append({
                        "id": sec_id, 
                        "vec_score": item["vec_score"],
                        "path": path_str,
                        "content": display_content,
                        "source": "VECTOR" 
                    })
                
                conn.close()
                
                if not self._is_interrupted and vector_candidates:
                    rerank_scores = self.rerank_with_bge(self.search_query, rerank_input_texts)
                    if rerank_scores:
                        for idx, candidate in enumerate(vector_candidates):
                            candidate['rerank_score'] = rerank_scores[idx]
                        
                        vector_candidates.sort(key=lambda x: x.get('rerank_score', 0), reverse=True)
                        self.log(f"âœ… Vector é€šé“å‡†å¤‡å°±ç»ª: {len(vector_candidates)} æ¡ (å·² Rerank)")

            # --- Wait for JSON Search ---
            json_results = []
            if self._json_worker:
                self.log("â³ ç­‰å¾… JSON åŸæ–‡ç¡¬æŸ¥è¯¢çº¿ç¨‹å®Œæˆ...")
                while self._json_worker.isRunning():
                    if self._is_interrupted:
                        self._json_worker.stop()
                        break
                    self._json_worker.wait(100) 

                json_results = self.json_search_results
            
            if self._is_interrupted:
                self.finish_signal.emit(False)
                return

            # --- RRF Fusion ---
            self.log("âš–ï¸ æ‰§è¡Œ RRF èåˆä¸å†…å®¹æŒ‡çº¹å»é‡...")
            final_top_results = self.apply_rrf_fusion(vector_candidates, json_results)
            
            final_top_results = final_top_results[:12]
            self.log(f"âœ… æœ€ç»ˆå¬å›: {len(final_top_results)} æ¡å”¯ä¸€å†…å®¹")
            
            for idx, res in enumerate(final_top_results):
                res['rank'] = idx + 1
                
            self.result_signal.emit(final_top_results)
            
            # --- LLM Summary ---
            self.call_deepseek_summary(self.original_query, final_top_results)
            
            if self._is_interrupted:
                self.finish_signal.emit(False)
            else:
                self.finish_signal.emit(True)

        except Exception as e:
            self.log(f"âŒ ä¸¥é‡é”™è¯¯: {str(e)}")
            import traceback
            self.log(traceback.format_exc())
            self.finish_signal.emit(False)